\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{url}
\usepackage{hyperref}
\usepackage{lipsum}
\usepackage{bm}
\usepackage{array}
\usepackage{cite}
\usepackage{listings}
\usepackage{gensymb}
\usepackage[justification=centering]{caption}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
    \definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=R,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}
    
\begin{document}

\title{Practical Software Reliability Modeling and Application: A Case Study
\thanks{This work is supported by NASA under contract number: }
}



\author{\IEEEauthorblockN{Vidhyashree Nagaraju, Shekar, Niti, and Lance Fiondella}
\IEEEauthorblockA{Department of Electrical and Computer Engineering\\
University of Massachusetts, Dartmouth, MA, 02747 USA\\
Email: \{vnagaraju,lfiondella\}@umassd.edu}
\and
\IEEEauthorblockN{Ying Shi}
\IEEEauthorblockA{Goddard Space Flight Center\\
National Aeronautics and Space Administration\\
Email: @}}

\maketitle

\begin{abstract}
This paper demonstrates the utility of a script to automatically apply a free and open source software failure and reliability assessment tool. The script generates a pdf report, eliminating the need to work with the user interface of the tool to manually assess the data and report. The reports can be used to summarize progress to both technical and non-technical leadership. Simplifying the assessment and reporting may encourage the software and reliability practitioners to apply the models and make decisions about the process.
\end{abstract}

\begin{IEEEkeywords}
Software reliability, software reliability growth model, R statistical programming language, GitHub, Software Failure and Reliability Assessment Tool, open source
\end{IEEEkeywords}


\section{Introduction}\label{sec:Intro}
Software reliability growth models (SRGM)~\cite{BookHoSRE} are extremely useful in making decisions about software development by characterizing failure data collected during testing. Some of the predictions featured by SRGM include the remaining number of failures, failure intensities, mean time to failures, release time, as well as overall software reliability. However, practitioners may be reluctant to apply these models due to a lack of either the knowledge of the underlying mathematics or the time to develop expertise and implement models in their work. Therefore, several computer-aided software reliability tools~\cite{trSMERFS,inProcISSRE2013_100,lyu1992casre} have been developed.

Previous computer-aided tools to apply software reliability methods include: Emerald~\cite{1996hudepohlemerald}, SRMP (Software Reliability Modeling Programs)~\cite{1988SRMP}, the AT\&T SRE Toolkit~\cite{1990ATT}, SoRel~\cite{1993kanounsorel}, SMERFS (Statistical Modeling and Estimation of Reliability Functions for Software)~\cite{trSMERFS}, and CASRE (Computer Aided Software Reliability Estimation)~\cite{lyu1992casre}, Robust~\cite{1995lirobust}, SREPT~\cite{2000ramanisrept}, CARATS~\cite{2011huangestimation}, SRATS~\cite{inProcISSRE2013_100}, and M-SRAT~\cite{2015shibatam}. Most of these tools are mostly spreadsheet based~\cite{inProcISSRE2013_100} or close source in nature. This inhibits the capability to integrate the existing tools into software testing work flows and update the tools with recent changes in the software reliability research. To overcome the limitation of existing tools, an open source Software Failure and Reliability Assessment Tool (SFRAT)~\cite{cFio53} is developed with a flexible architecture to accommodate individual researchers models as well as methods.

In this paper, we present a script to automatically apply an open source SFRAT and generate report. This script eliminates the need to work with the graphical user interface to manually prepare the report, thus conserving time. The script can be configured to produce custom reports; for example, the user can opt to include explanations of each result in the report. The script generates a pdf to visually assess the data and to ease the reporting process. The reports can then be used to summarize and visualize project status to both technical and non-technical leadership.

The remainder of the paper is organized as follows: Section~\ref{sec:SFRAT} provides a brief overview of the SFRAT user interface and Section~\ref{sec:Script} provides a detailed discussion of the script to generate the report automatically. Section~\ref{sec:Ex} demonstrates the use of the script through a real data, while Section~\ref{sec:Concl} provides conclusion and directions for future research.


\section{Software Failure and Reliability Assessment Tool (SFRAT)}\label{sec:SFRAT}
The Software Failure and Reliability Assessment Tool is a free and open source tool developed to promote quantitative assessment of software reliability as well as improved communications of such assessments. SFRAT is an application that estimates and predicts the reliability of a software system during test and operation. Some of the questions that the tool answers about a system undergoing test are:
\begin{itemize}
\item {Is the software reliable enough to release?}
\item {How long will it be before a specified goal is reached?}
\item {What will be the consequences if testing resources are insufficient?}
\end{itemize}

SFRAT is implemented in the R statistical programming language~\cite{}, an open source environment for statistical computing and graphics that can be used on computers running Windows, OSX, or Linux, with the user interface implemented using the R Shiny library~\cite{}. The code is accessible on GitHub at \textit{https://github.com/LanceFiondella/srt.core}, which after downloading may run using RStudio or some other R environment. In doing so, an organization can easily perform information assurance of the code prior to use on sensitive failure data. For complete details, the reader is referred to \textit{http://sasdlc.org/lab/projects/srt.html}.
%re-word sentence 3 below
The architecture of the SFRAT combines the use of existing software reliability models into a single tool, enabling more systematic comparison of models than previously possible. Presently, data formats supported consist of inter failure time, failure time, and failure count. Functionality includes two trend tests for reliability growth, two failure rate~\cite{BookHoSRE}, Jelinski-Moranda and geometric, and three failure counting models, Goel-Okumoto~\cite{goel1985software}, delayed S-shaped~\cite{artTR1986_19}, and Weibull~\cite{artNHPPsurvey} models as well as two measures of goodness of fit. Also implemented are methods to predict time to reach a desired reliability as well as detecting a number of future failures. The free and open source nature of the tool architecture enables additional models and goodness of fit measures to be later implemented. In this regard, the tool is intended to serve as a shared environment for collaboration among researchers and practitioners.

SFRAT can be accessed by downloading the source code from GitHub and run using an R environment (e.g. RStudio), or through the web instance. Once the application is launched, the initial SFRAT screen is shown as seen in Figure~\ref{fig_DefaultView}.
\begin{figure}[!h]
\centering
\includegraphics[width=\columnwidth]{Figures/DefaultView}
\caption{Initial View within SFRAT}
\label{fig_DefaultView}
\end{figure}


The SFRAT workflow is divided into four subtasks:
\begin{itemize}
\item{\textbf{Select, Analyze, and Filter Data}: Import failure data history and determine model applicability of its reliability's growth.}
\item{\textbf{Set Up and Apply Models}: View results of one or more reliability models, including reliability growth.}
\item{\textbf{Query Model Results}: Query applied models to answer the following questions:
\begin{itemize}
\item{How many additional failures will occur during a continued period of testing?}
\item{How much time would be required to observe a number of additional failures?}
\item{At what point in testing will a reliability goal be reached?}
\end{itemize}}
\item{\textbf{Evaluate Models}: By model comparison, determine which is most likely to proficiently characterize the data and provide accurate predictions.}
\end{itemize}


\subsection{Tab 1: Select, Analyze, and Filter Data}\label{tab1}
Figure~\ref{fig_Tab1_leftCol} shows the options in the first tab, which includes file selection, data visualization, and trend test analysis. The user can then use the tool by specifying the input file as either an Excel spreadsheet (.xlsx) or a CSV (comma separated value) (.csv) format. The input file must follow the format shown in Figure~\ref{fig_Excel_sys1}, where failure number(FN), interfailure time(IF), and failure time(FT) are indicated.

\begin{figure}[!h]
\centering
\includegraphics[width=3.4in]{Figures/Fig2}
\caption{Tab one view}
\label{fig_Tab1_leftCol}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[width=1.8in]{Figures/sys1excel}
\caption{Example Excel input}
\label{fig_Excel_sys1}
\end{figure}

Clicking on the \textbf{Browse...} button enables the user to search for and upload the input data file. The progress bar message ``Upload complete'' will indicate a successful file upload. By default, a plot of the cumulative failures for the uploaded data set is displayed. For example, Figure~\ref{fig_Tab1_CDF} shows the SYS1 data set~\cite{BookHoSRE}.

\begin{figure}[!h]
\centering
\includegraphics[width=3.4in, height=3in]{Figures/Fig4}
\caption{Tab one view after upload}
\label{fig_Tab1_CDF}
\end{figure}


From a drop-down menu below the progress bar, the user may choose from the provided data sets. CSV files may only contain only one data set, whereas Excel files will contain one data set per sheet. Data sets that do not comply with the input format will not be available in the dropdown menu. Regardless of the input's data format, the tool will provide conversions to failure time, failure count, and inter failure data formats upon upload, allowing for the use of any data model. $31$ data sets were taken from the Handbook of Software Reliability Engineering~\cite{BookHoSRE} and prepared in the file format. Of these, $10$ are failure time data and the remaining $21$ are failure counts. This enables a more comprehensive comparison between models than previously possible. Using the drop-down menu below ``Choose a view of the failure data'', the user may select an alternative data view, including failure intensities and the times between failures. In addition, the option exists to draw plots with either points, lines or both.

Data and Trend test plotting is also possible by use of the radio buttons with the same labels. These trend tests have been placed before model fitting to ensure that the data exhibits reliability growth and is therefore appropriate for software reliability models and predictions. The two tests implemented include the Laplace trend test~\cite{gaudoin1992optimal} as well as a running arithmetic average.

Figure~\ref{fig_Tab1_Laplace} shows the Laplace trend test of the SYS1 data set. The red line seen is a user-specified input, entered into the text box just underneath the Laplace Test drop-down box, as shown in Figure~\ref{fig_Tab1_leftCol}. In this case, the value has been set to $0.9$ or $90\%$. Additional default levels in black include $90$, $95$, $99$, $99.9$, $99.9999$, and $99.999999$. Values below these lines indicate that the data exhibits reliability growth with the specified level of statistical significance and is therefore suitable to apply software reliability models.

\begin{figure}[!h]
\centering
\includegraphics[width=3.4in]{Figures/SRT3}
\caption{Laplace trend test}
\label{fig_Tab1_Laplace}
\end{figure}


Figure~\ref{fig_SRT_RAT} shows the running arithmetic average of the SYS1 data set. If the time between failures increases, the running arithmetic average then increases, indicating an increase in system reliability. A decreasing running arithmetic average indicates reliability deterioration. Both the Laplace trend test and running arithmetic average suggest that the example SYS1 data set exhibits reliability growth.

\begin{figure}[!h]
\centering
\includegraphics[width=3.4in]{Figures/SRT_RAT}
\caption{Running arithmetic average}
\label{fig_SRT_RAT}
\end{figure}


Figure~\ref{fig:J4Trend} shows the Laplace trend test of the J4 data set, which does not experience any significant improvement in reliability. Thus, it is not appropriate to apply software reliability models to this data set until additional testing is performed to establish confidence in reliability growth. The slider at the bottom of Figure~\ref{fig_Tab1_leftCol} ranges from $1$ to $n$, where $n$ denotes the number of data points contained in a data set. The sliders allow the user to specify a subset of the data for plotting, model fitting, and prediction. The default is to use all $n$ data points for model fitting.

\begin{figure}[!h]
\centering
\includegraphics[width=3.4in,height=2in]{Figures/J4_Trend_LP}
\caption{Data set without reliability growth}
\label{fig:J4Trend}
\end{figure}

The user can switch between the \textbf{Plot} and~\textbf{Data and Trend Test Table} as shown in Figure~\ref{fig_Tab1_CDF}. Selecting table view displays the raw numerical data used to draw the plots in a tabular format similar to Figure~\ref{fig_Excel_sys1}. Selecting a radio button \textbf{.jpeg}, \textbf{.pdf}, \textbf{.png}, or \textbf{.tiff} (Figure~\ref{fig_Tab1_leftCol}) and then clicking \textbf{Save Display} saves a plot in the selected image format, while numerical tables may be saved as a CSV or PDF file. Tabular data may be exported to input into graphing software to include images in reports and research papers.


\subsection{Tab 2: Set up and Apply Models}\label{tab2}
Figure~\ref{fig_Tab2} shows the second tab's options, where model fitting will be performed. The first text box allows the user to ``Specify the number of failures for which the models will make predictions''. Here, the number of failures has been set to one for the sake of illustration. The second multi-select box allows the user to identify which models to fit with the data range chosen on Tab one. By default, the tool displays all available models. Clicking the \textbf{Run Selected Models} button executes the algorithms to fit the selected model.

\begin{figure}[!h]
\centering%SRT5
\includegraphics[width=3.4in]{Figures/Fig8}
\caption{Tab two options}
\label{fig_Tab2}
\end{figure}

Once model fitting completes, the multi-select box below ``Choose one or more sets of model results to display'' is populated with models that have completed successfully. Models that fail, however, will not be available. The results of successful models can be compared against the empirical data by selecting ``Cumulative Failures'', ``Time between failures'', ``Failure intensity'', or ``Reliability growth'' from the dropdown menu below ``Choose a plot type''.

Figure~\ref{fig_SYS1_Cum} shows a plot of the observed cumulative failures along with the fitted models. The black vertical line indicates the time at which the last failure occurred. Thus, points to its right indicate predicted failures. This line can be hidden by deselecting the \textbf{Show end of data on plot} checkbox. The legend at the bottom identifies the line that corresponds to each model fit. Figure~\ref{fig_SYS1_Cum} suggests that the geometric and Weibull models fit the observed data closely, whereas other models over-predict or under-predict the number of failures at various stages of testing.

\begin{figure}[!h]
\centering
\includegraphics[width=3.4in]{Figures/SRT6}
\caption{SYS1 cumulative failures and model fits}
\label{fig_SYS1_Cum}
\end{figure}


Figures~\ref{fig_TBF},~\ref{fig_FI}, and~\ref{fig_RG} show the times between failures, failure intensities, and reliability growth data views respectively as well as with the corresponding model fits.

\begin{figure}[!h]
\centering
\includegraphics[width=3.4in]{Figures/TBF}
\caption{SYS1 time between failures and model fits}
\label{fig_TBF}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[width=3.4in]{Figures/FI}
\caption{SYS1 failure intensity and model fits}
\label{fig_FI}
\end{figure}

\begin{figure}[!h]
\centering
\includegraphics[width=3.4in]{Figures/Reliability_growth}
\caption{SYS1 reliability growth and model fits}
\label{fig_RG}
\end{figure}


\subsection{Tab 3: Query Model Results}\label{tab3}
Figure~\ref{fig_Tab3} shows the options on the third tab, which focus on a variety of predictive algorithms. The multi-select box below ``Choose one or more sets of model results to display'' allows the user to specify a model with which they would like to make predictions. To determine the time required to observe the next $N$ failures, the user enters that number of failures in the text box just below.  Alternatively, the user may ``Specify the amount of additional time for which the software will run'' to determine the number of faults that would be detected a specified interval. Entering numbers into these text boxes automatically generates a table (for example see Figure~\ref{fig_Tab3-table}), which shows the model names and expected number of failures for the next $t$ time units as well as the expected time required to detect the next $N$ failures. In this case, Figure~\ref{fig_Tab3-table} shows predictions for the SYS1 data set, including the time for one additional failure and the predicted number of failures to be observed in the next $100,000$ seconds of additional testing time.

\begin{figure}[!h]
\centering
\includegraphics[width=3.4in]{Figures/Fig13}
\caption{Tab three options}
\label{fig_Tab3}
\end{figure}


\begin{figure}[!h]
\centering
\includegraphics[width=3.4in]{Figures/Tab3-table}
\caption{Failure predictions}
\label{fig_Tab3-table}
\end{figure}


Tab three (Figure~\ref{fig_Tab3}) also provides an option to estimate the testing time required to achieve a target reliability by entering the desired reliability and time for which the software must operate without failure in the text box below ``Specify the desired reliability'' and ``How much more test time to achieve a specified reliability?'' respectively.


\subsection{Tab 4: Evaluate Models}\label{tab4}
Figure~\ref{fig_SRT_tab4} shows tab four's options, which provide methods to assess model goodness of fit. The multi-select box below \textbf{Choose one or more sets of model results} allows a user to specify models they would like to apply goodness of fit measures, including the Akaike information criterion (AIC)~\cite{akaike1974new} and predictive sum of squares error (PSSE)~\cite{fiondella2011software}. The text box below ``Specify the Percent Data for PSSE'' allows specification of the fraction of data to be used to compute PSSE.

\begin{figure}[!h]
\centering
\includegraphics[width=3.4in]{Figures/Fig16}
\caption{Tab four options}
\label{fig_SRT_tab4}
\end{figure}

Figure~\ref{fig_SRT_tab4_table} shows the AIC and PSSE values for the five models applied to SYS1 when $90\%$ of data is used. The up/down arrows next to the performance measures in Figure~\ref{fig_SRT_tab4_table} sort the table based on rankings. Figure~\ref{fig_SRT_tab4_table} indicates that the Geometric and Weibull models perform best with respect to the AIC, while the Jelinski-Moranda and Goel-Okumoto models perform well with respect to PSSE.


\begin{figure}[!h]
\centering
\includegraphics[width=3.4in]{Figures/SRT_tab4_table}
\caption{AIC and PSSE of all models}
\label{fig_SRT_tab4_table}
\end{figure}


\section{SFRAT report generation script}\label{sec:Script}
This section discusses the installation and input specifications of the script. To reiterate, this script will automatically run the SFRAT based on user-defined variables inside of the script, and will then generate a PDF report of the exported data. As a result, it's simple to repeatedly create reader-friendly reports based on different data sets, for example. For some users, manually configuring the settings via the UI may be tedious and unpleasant, so therefore changing a few variables in a short script to perform the same action may be somewhat easier.
The report generation script is written in the R programming language, and uses its Markdown library to actually generate the report. To achieve this, two files are used, being \textbf{report-specifications.R} and
\textbf{SFRATReport.Rmd}. The \textbf{report-specifications.R} is used to configure the variables taken into account when generating the report, and \textbf{SFRATReport.Rmd} Markdown file is used to create reports in different formats.

\subsection{Script Installation}\label{sec:ScriptInstall}
To use the script, some user requirements must be met:
\begin{itemize}
  \item {\textit{Operating System}: Windows 7 (64-bit) or newer, Mac OS X 10.9 or newer, Linux capable of running R}
  \item {\textit{Perl}: Perl 5 version 16 or newer - may be pre-installed on OSX/Linux; for Windows, Perl may be downloaded from \textit{http://strawberryperl.com/}}
  \item{\textit{LaTeX}: Some user-chosen LaTeX editor (e.g. MikTex, TexStudio, Texmaker)}
  \item {\textit{R}: R(ver. 3.0 or newer) and RStudio(ver. 0.99.482 or newer) are required, downloadable from \textit{http://rstudio.com}}.
   \item{\textit{pandoc}: Newer versions of R does not support installing pandoc as a package, so it must be downloaded separately from \textit{https://pandoc.org/installing.html}.}
\end{itemize}

To download the script locally, navigate to \textit{http://github.com/LanceFiondella/SFRAT-Automated-Report}, and from the "Clone or download" drop-down menu click "Download ZIP". Extract the folder to a location of your choosing. RStudio should then be installed the same as any other application on the user's operating system. To be able to use the script, some R packages must be installed before use:
\begin{itemize}
	\item {\textbf{shiny}, a web application framework for R}
	\item {\textbf{gdata}, a package providing data manipulation tools}
	\item {\textbf{ggplot}, a graphical package capable of creating elegant and complex plots}
	\item {\textbf{DT}, a package providing an R interface to the JavaScript library DataTables}
	\item {\textbf{rootSolve}, a package used to find the roots of n nonlinear (or linear) equations}
	\item {\textbf{knitr}, a package providing a general-purpose tool for dynamic report generation in R using Literate Programming techniques}
	\item {\textbf{rmarkdown}, a package allowing converting R Markdown documents into a variety of formats including HTML, MS Word PDF, and Beamer}
	\item {\textbf{markdown}, a package for authoring HTML, PDF, and MS Word documents}
	\item {\textbf{readxl}, a package to import excel files}
	\item {\textbf{formatR}, a package designed to reformat R code to improve readability}
 \end{itemize}
 
These packages may be downloaded through RStudio or by use of another script included inside of the \textit{SFRAT-Automated-Report} folder. This may be done by opening the \textbf{installscript.R} file in RStudio and clicking "Source", similar to what's seen in Figure~\ref{fig:scriptsource}. To install via command line, run R from the script directory and use \textbf{source('installscript.R')} or \textbf{Rscript installscript.R}. The packages may also be installed manually via the "Install packages..." option in RStudio's "Tools" menu, as shown in Figure~\ref{fig:InstallPackages}:
	\begin{figure}[!h]
	\centering
	\includegraphics[width=3.4in]{Figures/InstallPackages}
	\caption{Install packages dialog box}
	\label{fig:InstallPackages}
	\end{figure}

\noindent To install a package manually, enter the specified package names into the text input, and make sure the "Install dependencies" box is checked. It's unnecessary to change the installation source or location.

\subsection{Using the report generation script}\label{sec:ScriptRun}
To run the main script using RStudio:
\begin{enumerate}
  \item {Launch RStudio and set its working directory to the location where the folder is saved.
  \begin{enumerate}
    \item {Click the menu options \textbf{Session} $\to$ \textbf{Set Working Directory} $\to$ \textbf{Choose Directory...}}
    \item {Navigate to the location of the downloaded folder and click `Open`.}
  \end{enumerate}
  }
  \item {Ensure that the required packages are installed. See Section ~\ref{sec:ScriptInstall} if not previously done.}
  \item {Open the file \textbf{report-specifications.R}. Each variable in this script represents some parameter in the UI for the SFRAT. Any setting found in the UI will be configurable inside this script. Below is a brief description of each parameter found in the script.
  \begin{itemize}
    \item {Main input specifications: 
    \begin{itemize}
    \item {\textbf{verbose\_report} - Enabling this setting will show verbose reports on the PDF report. It provides a brief description of the purpose of each figure.}
      \item {\textbf{sheetNumber} - If the input data file has multiple data sets arranged on different sheets, this value allows the user to decide which data set to use. It's encouraged to name the data sets as that name will appear on the report.}
      \item {\textbf{datapath} - This specifies the path of the input data file.}
      \item {\textbf{colors} - This specifies the set of colors used in the report. If nothing is specified, the default set of colors will be used to display graphical results.}
    \end{itemize}
  }
    \item {Tab 1: Select, Analyze, and Filter Data
    \begin{itemize}
    \item{\textbf{confidence\_lvl} - This allows the user to specify a confidence level between $0$ and $1$ for the Laplace trend test to quantify a desired level of significance for reliability growth.}
    \end{itemize}
    }
    \item {Tab 2: Set Up and Apply Models
    \begin{itemize}
      \item {\textbf{num\_failures\_future\_prediction} - The user can specify the number of failures that they would like to predict beyond the end of testing.}
      \item {\textbf{models\_to\_apply} - This allows the user to specify which software reliability growth models they would like to apply to make predictions. The available models are a delayed s-shape (DSS), geometric (GM), Goel-Okumoto (GO), Jelinski-Moranda (JM), and Weibull (Wei) model. }
      \item {\textbf{mission\_time} - The user can specify what amount of mission time beyond the testing period for which they would like to view the reliability growth trend. }
    \end{itemize}
    }
    \item {Tab 3: Query Model Results
    \begin{itemize}
      \item {\textbf{num\_failures\_to\_predict} - Specify the number of failures to predict beyond the end of testing. This is similar to \textit{num\_failures\_future\_prediction}.}
      \item {\textbf{additional\_time\_software\_will\_run} - User can specify the additional time beyond the end of testing to predict the number of failures. }
      \item {\textbf{desired\_reliability} - User can specify the target reliability between $0$ to $1$ to estimate the time required to achieve such failures.}
      \item {\textbf{reliability\_interval\_length} - This is used to specify the mission time beyond testing to estimate the reliability.}
    \end{itemize}
    }
    \item {Tab 4: Evaluate Models
    \begin{itemize}
    \item{\textbf{percent\_data\_for\_PSSE} - The user can specify the percentage of data to be used for model fitting. The remaining data will be used to assess model prediction capability using the predictive sum of squares error.}
    \end{itemize}
    }
  \end{itemize}
  }

  \item{After specifying the input parameters, save the file. To modify the header of the report PDF (e.g. author name), modify lines $2$ through $6$ in \textbf{SFRATReport.Rmd}. There are two methods to run the script and generate the report; either click \textit{Source} in the top-right corner as shown in Figure~\ref{fig:scriptsource}, or in the console, run the following command: \textbf{source(‘report-specifications.R’)}
	\begin{figure}[!h]
	\centering
	\includegraphics[width=3.2in]{Figures/scriptsource}
	\caption{Source script file}
	\label{fig:scriptsource}
	\end{figure}
}

  \item {The generated reports will be stored in the \textit{Report} folder, which is located in the same directory as the script. Each report file will have the following naming convention: 
  \newline
  \textbf{SFRAT report\_dataName\_YYYY-MM-DD.pdf}}
\end{enumerate}
The script may also be easily run via the command line. Assuming that all prerequisites are met, navigate to the script folder, run R and enter \textbf{source('report-specifications.R')}, or simply run \textbf{Rscript report-specifications.R} in the script directory.

\section{Illustrations}\label{sec:Ex}
This section displays the example results based on the SYS1 example data~\cite{BookHoSRE}. The second example demonstrates the online assessment of the data using SYS1.

\subsection{Script on SYS1 data}\label{sec:Ex:Script}
This example demonstrates the use of the script in the context of SYS1 example data, which consists of $136$ failures. The in-line comments have been removed as variable summaries are available in Section \ref{sec:ScriptRun}. This example purely displays the example values used to generate the given report. The following variables used work best with the SYS1 data set.
\begin{lstlisting}
verbose_report <- TRUE								
sheetNumber <- 1
filePath <- '/SFRAT/model_testing/model_data.xlsx'
colors <- c("navy","red","green","firebrick4","magenta")
confidence_lvl <- 0.9
num_failures_future_prediction <- 2	
models_to_apply <- c('DSS', 'GM', 'Wei','GO','JM')
mission_time <- 600
num_failures_to_predict <- 2
additional_time_software_will_run <- 4116
desired_reliability <- .9
reliability_interval_length<- 600
percent_data_for_PSSE <- .9
	
\end{lstlisting}

Figure~\ref{fig:SYS1Report} shows the initial view of the SFRAT report generated, displaying an initial view of some of the input SYS1 data. This provides a verification as well as a sample of the data used.
\begin{figure}[!ht]
\centering
\includegraphics[width=3.4in]{Figures/SYS1Report}
\caption{Initial view of the report using SYS1 data}
\label{fig:SYS1Report}
\end{figure}

When viewing the PDF, the left side of the report (as demonstrated in Figure~\ref{fig:SYS1Report}) will give a outline of the different sections of the report. This particular report is presented over $12$ pages, with each result using a separate page. With verbose reporting enabled, a brief description of each graph or table is also presented. In addition, the first page allows the user to specify the author, report name, date, and time. If not previously done, modify the first few lines in \textbf{SFRATReport.Rmd} inside the script folder and re-generate the report.

To describe the report shortly, graphs for times between failures and failure intensity as a function of the testing time is displayed on pages $3$ and $4$ respectively. Similar plot and discussion for the running arithmetic average is included in the following page.

%\subsection{Online analysis using the script on SYS1 data}\label{sec:Ex:ScriptOnline}
%example of application sequence that enables comparison of which model predicts best throughout the data

%collection process and the assessments that might have been made if this were an ongoing activity. We can do %this in the

%context of sys1 and then adapt to NASA data.


\section{Conclusion and Future Work}\label{sec:Concl}
In this paper, we presented a script to automatically apply the Software Failure and Reliability Assessment Tool. An example of the SFRAT in the context of SYS1 dataset and a detailed description to run the tool is discussed. Automated PDF report generation is demonstrated using SYS1 data set. A second example is presented to illustrate the utilization of script to perform online assessment of the data. 

Future work will extend this script to accommodate the updated version of the SFRAT. Both the SFRAT and the script will be ported to the Python programming language, considering its ease of use. In addition, options to perform online data assessment will be integrated with required functionality and measures.


\section*{Acknowledgment}\label{sec:Ack}
This work is supported by the National Aeronautics and Space Administration (NASA) and SARP under Grant Number (\#). The authors would also like to acknowledge Christian Ellis, Joshua Steakelum, and Melanie Luperon for their inputs on the manuscript.


\bibliographystyle{IEEEtran}
\bibliography{bibIEEE}

\end{document}
